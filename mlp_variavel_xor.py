# -*- coding: utf-8 -*-
"""MLP variavel XOR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_kdGyDRLLkmgh4n63mJiADbJyj65Nr3C
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#uploaded = files.upload()

rnd_seed = np.random.seed(10)

data = {'y': [0, 1, 1, 0], 'x1': [0, 0, 1, 1], 'x2': [0, 1, 0, 1]}

dataset = pd.DataFrame(data=data)
test_dataset = dataset

# dados de entrada
# verificar notação no livro do haykin, capítulo 4
# número de instancias de treinamento por época
num_inst = len(dataset.index)

#número de épocas
n_epoch = 1000

# número de dados de treinamento
N = num_inst*n_epoch
N2 = int(N / 10) # número reduzido, para plotar gráficos


# Profundidade de rede
L = 3

# Nós de entrada
m0 = 2





# camada de entrada, l=1
m1 = 6 # quantidade de neurônios na camada


# camada de entrada, l=2
m2 = 2 # quantidade de neurônios na camada

# camada de saída l=L
mL = 1 # quantidade de neurônios na camada
m = [m0, m1, m2, mL]


acertividade = np.zeros(n_epoch)
Eav = np.zeros(n_epoch)






y = [None] * L # ativação do neurônio
d = [None] * L # saídas desejadas
v =[None] * L # vetor de valores antes da ativação do neurônio
w = [None] * L # vetor de pesos
delta = [None] * L
e = [None] * L
x = [None] * L # sinais de entrada



for l in range(0,L):
  y[l]     = np.ones(m[l+1]) # ativação do neurônio
  v[l]     = np.ones(m[l+1]) # vetor de valores antes da ativação do neurônio
  w[l]     = np.ones((3, m[l+1], m[l] + 1)) # vetor de pesos
  delta[l] = np.ones(m[l+1])
  e[l]     = np.ones(m[l+1])
  d[l]     = np.ones(m[l+1]) # saídas desejadas
  x        = np.ones(m[l+1] + 1) # sinais de entrada

eta = [None] * L
alpha = [None] * L
b = [None] * L
a = [None] * L

#taxa de aprendizado
eta[L-1] = np.ones(N) * list(np.linspace(0.8, 0.002, N))
eta[1] = np.ones(N) * list(np.linspace(0.8, 0.002, N))
eta[0] = np.ones(N) * list(np.linspace(0.8, 0.002, N))

# Taxa de aprendizado com decaimento exponencial
# np.ones(N) * (list(0.9*np.exp(-5* np.linspace(0.0, 1., N))+0.005))

# Taxa de aprendizado constante
# etaL = np.ones(N)
# etaL[0:int(N/4)] *= 0.9
# etaL[int(N/4):N] *= 0.9


# parâmetro da função de ativação
a[L-1] =  0.95#1.7159
a[0] =  0.9#1.7159
a[1] = 0.8# 1.7159

b[L-1] = 1
b[0] = 1
b[1] = 1
# termo momentum 
alpha[L-1] = np.ones(N) * 0.00001
alpha[0] = np.ones(N) * 0.00001
alpha[1] = np.ones(N) * 0.00001



yn = [[None] * N2] # ativação do neurônio
dn = [[None] * N2] # saídas desejadas
vn = [[None] * N2] # vetor de valores antes da ativação do neurônio
wn = [[None] * N2] # vetor de pesos
deltan = [[None] * N2]
en = [[None] * N2, [None]*L]

for n in range(0,N2):
  yn[n] = [[None] * L]  # ativação do neurônio
  vn[n] = [[None] * L]  # vetor de valores antes da ativação do neurônio
  wn[n] = [[None] * L]  # vetor de pesos
  deltan[n] = [[None] * L]
  en[n] = [[None] * L]
  dn[n] = [[None] * L] # saídas desejadas

for n in range(0, N2):
  for l in range(0,L):
    yn[n][l] = np.ones(m[l] + 1)  # ativação do neurônio
    vn[n][l] = np.ones(m[l] + 1) # vetor de valores antes da ativação do neurônio
    wn[n][l] = np.ones((m[l + 1], m[l] + 1)) # vetor de pesos
    deltan[n][l] = np.ones(m[l] + 1)
    en[n][l] = np.ones(m[l] + 1)
    dn[n][l] = np.ones(m[l] + 1) # saídas desejadas

plt.plot(eta[L-1])
plt.plot(eta[2])
plt.plot(eta[1])
plt.title('Taxa de aprendizado')

# Inicialização dos vetores

w[0][0] = np.random.rand(m1, m0 +1)*1.0 -0.5
w[1][0] = np.random.rand(m2, m1 +1)*1.0 -0.5
w[2][0] = np.random.rand(mL, m2 + 1)*1.0 - 0.5

w[0][1] = np.random.rand(m1, m0 +1)*1.0 -0.5
w[1][1] = np.random.rand(m2, m1 +1)*1.0 -0.5
w[2][1] = np.random.rand(mL, m2 + 1)*1.0 - 0.5

# Inicialização do bias como zero
for l in range(0,L):
  for j in range(0,m[l+1]):
    w[l][0][j][m[l]] = 0
    w[l][1][j][m[l]] = 0



# inicialização das variáveis de controle
count_n=0

def activation_func(a , b , v):
  #return 1/(1+ np.exp(-a * v))
  return a * np.tanh(b * v)

def d_func_ativacao(a , b, v):
  # return (a * np.exp(-a * v)) / ((1 + np.exp(-a * v))**2)
  return a * b * (1 - np.tanh(b * v)**2)

print(m[3])

# treinamento da rede neural
count_epoch = 0

# for ne in range(0,n_epoch):
ne=0
n2=0
while (ne < n_epoch):
  
  
  dataset_shufle = dataset.sample(frac = 1,random_state=rnd_seed, axis=0)
  
  
  
  for ni in range(0, num_inst):
    n = ni + ne*(num_inst)
    if n >= (N-1) :
      break

  
  # progagação do sinal forward
    # Extrai a imagem da base de dados
    local_image_array = np.array(dataset_shufle.iloc[ni,1:(m0+1)])
    
    #local_image_array = np.where(local_image_array == 0, -1, local_image_array)

    #print(local_image_array)
    # Acrescenta 1 que é relativo ao bias
    local_image_array = np.append(local_image_array, 1)
   
    # print(f'yl1 = {y[0]}')
    # print(f'yl2 = {y[1]}')
    # print(f'ylL = {y[2]}')

    for l in range(0,L):
      if l == 0:
        yb = local_image_array
      else: 
        yb = np.append(y[l-1], 1)
        
    # percorre todos os neurônios da camada 1
      for j in range(0,m[l+1]):
        #print(f'l={l}, m = {j}/{m[l+1]}, y[l][j]={y[l][j]}')
        
        temp = w[l][1][j]
        # print(f'w[l][1][j]={w[l][1][j]}, yb={yb}')
        v1 = np.matmul(np.transpose(temp),yb)
        
        y[l][j] = activation_func(a[l], b[l], v1)

      # Acrescenta 1 que é relativo ao bias


    

    # # vetor temporário para armazenar as saídas desejadas
    d = dataset.iloc[ni,0]
    
    # back propagation
    for l in range(L-1,-1,-1):
      if l == 0:
        yb = local_image_array
      else:
        yb = np.append(y[l-1], 1)

      # percorre todos os neurônios da camada de saída
      for j in range(0,m[l+1]):
        # print(f'l = {l}, m = {m[l+1]}')
        
        #print(f'{w[l][1][j]}')
        v1 = np.matmul(np.transpose(w[l][1][j]), yb)
        y[l][j] = activation_func(a[l], b[l], v1)
        
        # Início do back propagation

        # Calcula erro
        if l == (L-1):
          e[l][j] = d - y[l][j]
        else:
          # Array com os pesos da camada de saída em relação ao neurônio 
          # da camada oculta
          wlLkj = np.zeros(m[l+2])
          
          for k in range(0,m[l+2]):
            

            wlLkj[k] = w[l+1][1][k][j]

          e[l][j] = np.sum(delta[l][l] * wlLkj)

        # Calcula gradiente local
        delta[l][j] = e[l][j] * d_func_ativacao(a[l], b[l], v[l][j])

        # Atualiza os pesos na camada de saída
        if n == 0:
          w[l][2][j] = w[l][1][j] +  eta[l][n] * delta[l][j] * yb
        else:
          w[l][2][j] =  w[l][1][j] + alpha[l][n] * w[l][0][j] + eta[l][n] * delta[l][j] * yb
        
        w[l][0][j] = w[l][1][j]
        w[l][1][j] = w[l][2][j]


    if (n % 10) == 0: 
      # print(wn)
      for l in range(0,L):
        print(n2)
        # wn[n2][l] = np.copy(w[l][1])
      # en[n2] = np.copy(e)
      n2 = n2+1

    if (n % 1000) == 0:    
      print(f'n = {n}/{N}, Época = {ne}/{n_epoch}')
  
  Eav[ne] = 1/(2+N)*np.sum(np.sum(e[L-1][(ne * num_inst) : ((ne+1) * num_inst)] ** 2))
  if Eav[ne] < Eav[ne-1]:
    sentido = "Down"
  else:
    sentido = "up"
  print(f'Época: {ne}/{n_epoch}, Acertividade: {acertividade[ne]:.2f}%, Eav = {Eav[ne]:.8f} {sentido}')
  ne+=1
    
plt.figure()
plt.plot(Eav[0:-2])
plt.figure()
plt.plot(acertividade)

print(w[2][0])
plt.plot(Eav)

elLn1 = np.zeros((N2,m0+1))


for n in range(0,N2):
    elLn1[n][0] = en[L-1][0]
  

plt.plot(elLn1[0:N])
#plt.plot(eta[2][0:N])
print(elLn1[0:300])

#teste da rede neural
for n in range(0,num_inst):
  x1 = dataset.iloc[n,1]
  x2 = dataset.iloc[n,2]
  d =  dataset.iloc[n,0]
  plt.scatter(x1,x2, marker = f'${int(d)}$',s=200)
  
  #yout = np.sign(np.matmul(np.transpose(wlL[N-1]),x[n]))
  #print(f'{x[n][1]} & {x[n][2]} = {yout}')

# w[N-1][0] = -1.8
# w[N-1][1] = 1.
# w[N-1][2] = 1.





x_space = np.linspace(0,1,10)


for i in range(0,m2):
  bwl21 = wl2[N-1][i][2]
  w1l21 = wl2[N-1][i][0]
  w2l21 = wl2[N-1][i][1]
  cy1 = (-bwl21 - w1l21 * x_space) / w2l21
  plt.plot(x_space,cy1)

#print(cy2)
plt.show()

w0lL = np.zeros((N,m2+1))


for n in range(0,N):
  for j in range(0,m2+1):
    w0lL[n][j] = wlL[n][0][j]
  

plt.plot(w0lL)
#print(w0lL)
plt.plot(etaL)

w0l2 = np.zeros((N,m0+1))


for n in range(0,N):
  for j in range(0,m0+1):
    w0l2[n][j] = wl2[n][0][j]
  

plt.plot(w0l2)
plt.plot(etal2)

w1l2 = np.zeros((N,m0+1))


for n in range(0,N):
  for j in range(0,m0+1):
    w1l2[n][j] = wl2[n][1][j]
  

plt.plot(w1l2)
plt.plot(etal2)

# Teste da rede neural
def teste_rede(x_data):
  # percorre todos os neurônios da camada 1
  x = np.copy(x_data)
  x = np.append(x,1)  
  n = N-1

  for l in range(0,L):
      if l == 0:
        yb = x
      else:
        yb = np.append(y[l-1], 1)
        
    # percorre todos os neurônios da camada 1
      for j in range(0,m[l+1]):
        #print(f'l={l}, m = {j}/{m[l+1]}, y[l][j]={y[l][j]}')
        # print(f'l={l}, yb={yb}')
        temp = w[l][1][j]
        v1 = np.matmul(np.transpose(temp),yb)
        
        y[l][j] = activation_func(a[l], b[l], v1)
  
  
  return_value=0

  for j in range(0,m[L]):
    
    if y[L-1] > 0.8:
        return_value = 1
  print(y[L-1])
  return return_value
  
cont_acert = 0
for i in range(0,num_inst):
  x = list(dataset.iloc[i,1:(m0+1)])
  d =  dataset.iloc[i,0]
  y_rede = teste_rede(x)
  print(f'x = {list(x)}, y = {y_rede}, d = {d}')
  if (np.sign(y_rede) == np.sign(d)):
    cont_acert += 1

if cont_acert == 4 :
  print("Passou!")
else:
  print(f'Não passou ... {cont_acert}/4')

a1 = np.array([0,1,2])
b1=np.copy(a1)
print(b1)